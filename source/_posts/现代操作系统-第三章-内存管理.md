---
title: 现代操作系统-第三章-内存管理
top: false
cover: false
toc: true
mathjax: true
date: 2020-09-10 09:43:36
password:
summary:
tags:
categories: 操作系统
thumbnail:
---

虽然上一章说要挑自己感兴趣的看，但发现知识储备并不支持我这么做，所以还是按顺序看了下去。

内存（RAM）是计算机中重要且基本的一个组成部分，需要认真管理。每个程序员都梦想拥有这样的存储器：它是私有的、内容无限大的、速度无限快的、永久性的（即断电时不会丢失数据），并且价格低廉。遗憾的是，目前这样的存储器还不存在。现实中，我们使用的是分层存储器体系（memory hierarchy）的概念。
在这个体系中，计算机有若干MB快速、昂贵且易失的高速缓存（cache），若干GB速度与价格适中但易失的内存，以及若干TB低速、廉价、非易失的磁盘存储，另外还有U盘和记忆卡等可移动存储装置。操作系统的工作是将这个存储体系抽象为一个有用的模型并管理这个抽象模型。
操作系统中管理分层存储体系的部分称为存储管理器（memory manager）。它的任务是有效地管理内存，即记录哪些内存是正在使用的，哪些内存是空闲的；在进程需要时为其分配内存，在进程使用完后释放内存。

<!--more-->

## 无存储器抽象
最简单的存储器抽象就是根本没有抽象，即每个程序都直接访问物理内存。
在这种情况下，想要在内存中同时运行两个程序是不可能的。如果第一个程序在2000的位置写入一个新的值，将会擦掉第二个程序存放在相同位置上的所有内容，因为程序直接指定了物理内存的位置，所以同时运行两个程序行不通。

不过也存在一些变通的方法。

![](/images/现代操作系统/系统进程和用户进程使用不同的内存位置.png)

这样的变通方法允许操作系统进程运行的同时，再运行一个用户进程。第一种方案以前被用在大型机和小型计算机上，现在很少使用了。第二种方案被用在一些掌上电脑和嵌入式系统中。第三种方案用于早期的个人计算机中（例如运行MS-DOS的计算机），在RAM中的系统部分称为BIOS（Basic Input Output System）。第一种和第三种方案的缺点是用户程序出现的错误可能摧毁操作系统，引发灾难性的后果。
在没有存储器抽象的系统中实现并行的一种方法是使用多线程来编程。虽然这个想法行得通，但没有被广泛使用，因为人们通常希望能够在同一时间运行没有关联的程序，而这是线程抽象所不能提供的。

**在不使用存储器抽象的情况下运行多个程序**
更进一步的方法是，将当前内存中所有内容保存到磁盘文件中，然后把下一个程序读入内存运行，只要同一时间中只有一个程序在运行，就不会发生冲突。这么做的弊端是显而易见的，慢，但是这个交换的思想在后面会继续讨论。
另外是借助特殊的硬件来实现。IBM 360的早期模型是这样解决的：内存被划分为2KB的块，每一块被分配一个4位的保护键，保护键存储在CPU的特殊寄存器中。运行中的程序如果访问了不属于它的内存地址，那么该地址的保护键和程序状态字中的保护键就会冲突，360的硬件就会捕获到这一事件，从而防止进程之间互相干扰。
然而，这仍然没有解决程序访问绝对物理地址而造成互相之间冲突的问题。IBM 360使用静态重定位技术：当一个程序被装载到地址16384时，常数16384会被加到每一个程序地址上。但这不是一种通用的解决方法，同时会减慢装载速度，而且它要求给所有的可执行程序提供额外的信息来区分哪些内存字中存有（可重定位的）地址，哪些没有，比如
```
MOV REGISTER1, 28
```
这样把数28送到REGISTER1的指令不可以被重定位。

虽然直接引用物理地址对于大型计算机、小型计算机和个人电脑来说已经是很久远的记忆了，但是这种缺少存储器抽象的情况在嵌入式系统和智能卡系统中还是很常见的，比如收音机、洗衣机、微波炉这样的设备已经完全被（ROM形式的）软件控制，因为它们运行的所有程序都是可以事先确定的。

## 一种存储器抽象：地址空间
把物理地址暴露给进程会带来以下几个严重问题：
1. 如果用户程序可以访问内存的每个地址，那么它们就可以很容易地破坏操作系统，造成系统崩溃
2. 同时运行多个程序是非常困难的，而这种需求十分常见
因此，我们需要想想其他办法。

### 地址空间的概念
要使多个应用程序同时处于内存中且相互不影响，需要解决两个问题：保护和重定位。一个比IBM 360使用的静态重定位技术更好的方法是创造一个新的存储器抽象：地址空间。
就像进程的概念创造了一类抽象的CPU以运行程序一样，地址空间为程序创造了一种抽象的内存。地址空间是一个进程可用于寻址内存的一套地址集合。每个进程都有自己的地址空间，并且这个地址空间独立于其他进程的地址空间。

**基址寄存器与界限寄存器**
这个简单的解决办法是使用动态重定位，简单地把每个进程的地址空间映射到物理内存的不同部分。我们为每个CPU配置两个特殊的硬件寄存器，通常叫做基址寄存器和界限寄存器。
当使用基址寄存器和界限寄存器时，程序装载到内存中连续的空闲位置且装载过程中无需重定位

![](/images/现代操作系统/基址寄存器和界限寄存器.png)

每次进程访问内存，取一条指令，读或者写一个数据字，CPU硬件会把地址发送到内存总线前，自动把基址寄存器中的值加到该地址值上，同时检查它是否等于或大于界限寄存器中的值。这样，对于图中白色部分的程序第一条指令
```
JMP 28
```
就会被硬件解释成
```
JMP 16412
```
这样，程序就如我们所愿地跳转到了CMP指令。
使用基址寄存器和界限寄存器重定位的缺点是，每次访问内存都需要进行加法和比较运算，比较运算可以很快，但加法运算在没有使用特殊电路的情况下会显得很慢。

### 交换技术
如果计算机物理内存足够大，那么之前所提及的所有方案多多少少是可行的，但实际上，内存总是不够用。一个典型的Windows、OS X或Linux系统，在计算机完成引导后会启动50～100个甚至更多的进程，而仅仅一个Photoshop用户程序一启动就轻易地占据了500M内存，而开始处理数据后可能需要数GB的空间。因此，把所有进程保存在内存中需要巨大的内存，如果内存不够，就做不到这一点。
有两种处理内存超载的通用方法。最简单的策略是交换（swapping）技术，即把一个进程完整调入内存，使该进程运行一段时间，然后把它存回磁盘。另一种策略是虚拟内存（virtual memory），该策略甚至能使程序在只有一部分被调入内存的情况下运行。
下面讨论交换策略。

![](/images/现代操作系统/交换内存.png)

交换在内存中产生了多个空闲区（hole，也称为空洞），通过把所有的进程尽可能向下移动，有可能将这些小的空闲区合成一大块，该技术被称为内存紧缩（memory compaction），通常不进行这个操作，因为很费CPU时间。
如果进程在创建后所需内存大小不变，则分配时很简单，系统按需分配就是了，但是如果进程在运行时试图增长，就会导致问题。如果该进程相邻区域空闲，则可以扩大它的空间，若其相邻区域已被其他进程占据，则需要将其移动到一块内存中足够大的区域去，若内存已满，则需要将若干其他进程换走，直到空出足够大的空间。显然，这么做很费时。

![](/images/现代操作系统/预留内存空间.png)

一种解决方法是在分配时预留一段供其增长的数据段。

### 空闲内存管理
在动态分配内存时，操作系统必须对其进行管理。一般而言，有两种方法跟踪内存使用情况：位图和空闲链表。

**使用位图的存储管理**
位图法要求将内存划分成几个字节到几千字节的分配单元，每个分配单元对应位图中的一位，用0和1表示空闲和占用。

![](/images/现代操作系统/位图和链表.png)

分配单元的大小是一个重要的设计因素。分配单元越小，位图越大，实际可用内存就越小，但分配单元越大，浪费的内存也越大，因为只能分配整数个单元。
另外，在决定把一个占k个分配单元的进程调入内存时，存储管理器必须搜索位图，在位图中找出有k个连续0的串，这是个耗时的操作。

**使用链表的存储管理**
另一种记录内存使用情况的方法是，维护一个记录已分配内存段和空闲内存段的链表。如上图c所示。
当系统要为进程分配内存时，会沿着链表进行搜索，直到找到一个足够大的空闲区。
最简单的算法是首次适配法（first fit），找到的第一个足够大的空间就供进程使用，该空间剩余的部分作为新的空闲区。首次适配法速度较快，因为它尽可能少地搜索链表结点。
另一个算法时最佳适配法（best fit），它搜索整个链表，选择能够容纳进程的最小的空闲区。它比首次匹配法要慢，因为搜索整个链表，但出乎意料的是，它反而比首次匹配法更浪费内存，因为它会产生大量无用的小空闲区域。
此时，作为一名刷过算法的程序员，能敏锐地察觉到对这两种算法有不少加速的可能，比如为进程和空闲区各自维护独立的链表，这样就能只检查空闲区链表。另外还能对链表排序，这样就进一步缩短了搜索的时间。
还有一种算法称为快速适配（quick fit），它为常用大小的空闲区维护单独的链表，比如4KB一个链表，8KB一个链表，以此类推，这样系统就可以根据需要，选择合适的大小来用。

## 虚拟内存
虽然交换理论上来说可行，但实际上并不是一个具有吸引力的方案，因为一个典型的SATA磁盘的峰值传输大概是每秒几百兆，这意味着需要好几秒才能换入或换出一个1GB的程序。
20世纪60年代采取的一种解决方案是：把程序分割成许多片段，称为覆盖（overlay）。分割后的程序依次加载进入内存，以此做到小内存运行大程序。但注意，这个分割是由程序员完成的，这通常十分枯燥且复杂，显然，这种做法很快被淘汰了。
因此，一种新方法产生了，虚拟内存（virtual memory）。虚拟内存的基本思想是：每个程序拥有自己的地址空间，这个空间被分割成很多块，每一块称作一页或页面（page）。每一页有连续的地址范围，这些页被映射到物理内存，但并非所有页都在内存中才能运行程序，当程序引用到在物理内存中的地址空间时，硬件立刻执行必要的映射，当引用到不再物理内存中的地址空间时，由操作系统负责将缺失的部分装入内存并重新执行指令。
虚拟内存很适合在多道程序设计系统中使用，许多程序的片段保存在内存中，当一个程序等待它的一部分读入内存时，就可以把CPU交给另一个进程使用。

### 分页
大部分虚拟内存系统中都使用一种称为分页（paging）的技术。当程序执行指令
```
MOV REG, 1000
```
时，它把地址为1000的内存单元内容复制到REG中（或者相反，这取决与计算机的型号）。在没有虚拟内存的计算机上，系统直接将该地址送到内存总线上，读写相应的物理地址；而在使用虚拟内存的情况下，虚拟地址被送到内存管理单元（Memory Management Unit，MMU），MMU把虚拟地址映射为物理内存地址。

![](/images/现代操作系统/虚拟内存.png)

但是这并没有解决虚拟内存地址空间比物理内存大的问题。在实际的硬件中，用一个“在/不在”位（present/absent bit）记录页面在内存中的实际存在情况。如果程序访问了一个“不在”的页面，MMU会注意到该页面没有被映射，于是使CPU陷入操作系统，这个陷阱被称为缺页中断或缺页错误（page fault）。操作系统找到一个很少使用的页框（物理内存中对应的单元，page frame）把它写入磁盘，然后把需要访问的页面读到刚才回收的页框中，修改映射关系，最后重新启动引起陷阱的指令。

### 页表
页表的目的是把虚拟页面映射为页框，从数学角度说，页表是一个函数，它的参数是虚拟页号，结果是物理页框号。

**页表项的结构**
![](/images/现代操作系统/页表项.png)

不同计算机的页表项大小可能不一样，但32位是一个常用的大小。最重要的是页框号，其次是“在/不在”位。保护位指出一个一个页允许什么类型的访问。最简单的形式是这个域只有一位，0表示读/写，1表示只读。为了记录页面的使用状况，引入了修改位和访问位。修改位也被称为脏位，它表示该页面是否被修改过，修改过的页面需要重新写回磁盘，而未修改过的直接丢弃即可。访问位记录该页是否被访问过，方便操作系统决定淘汰哪一个页面。最后一位用于禁止页面被高速缓存，通常用于虚拟内存映射I/O设备，我们希望拿到的是最新的I/O设备的值而不是一个旧的被缓存的值，因此禁止它被缓存从而使CPU必须访问内存。
需要注意的是，页表不保存某页对应的磁盘地址，这个信息是操作系统自己内部保存的。

虚拟内存的本质是用来创造一个新的抽象概念--地址空间，这个概念是对物理内存的抽象，类似于进程是对物理处理器（CPU）的抽象。虚拟内存的实现，是将虚拟地址空间分解成页，并将每一页映射到物理内存的某个页框。

### 加速分页过程
之前了解了虚拟内存和分页的基础，现在来谈实现。在任何分页系统中，都需要考虑两个主要问题：
1. 虚拟地址到物理地址的映射必须非常快
2. 如果虚拟地址空间很大，那么页表也会很大

第一个问题是因为每次访问内存都需要进行虚拟地址到物理地址的映射。如果执行一条指令需要1ns，那么页表查询必须在0.2ns之内完成，以免映射成为一个主要瓶颈。
第二个问题来自计算机的地址空间大小。现代计算机使用至少32位的虚拟地址，而且64位变得越来越普遍。假如页面大小为4KB，32位的地址空间将有100万页，而64位的地址空间多到超乎想象。并且，每个进程都需要自己的页表。

最简单的设计，是使用由“快速硬件寄存器”阵列组成的单一页表，每一个表项对应一个虚拟页面，虚拟页号作为索引。

![](/images/现代操作系统/快速硬件寄存器.png)

当启动一个进程时，操作系统把保存在内存中的进程页表加载到寄存器中，这样在运行进程的时候就不必在访问内存。然而，这么做的缺点在于，当页表很大时，代价高昂，而且每次上下文切换都必须装在整个页表，这样会降低性能。
另一种极端的方法是，页表都在内存中，寄存器中只保存指向页表的起始位置，显然这样在执行每条指令时，都要一次或多次访问内存来完成页表项的读入，速度非常慢。

**转换检测缓冲区**
一种解决方案是为计算机设置一个小型的硬件设备，将虚拟地址直接映射到物理地址，从而不必再访问内存中的页表。这种设备称为转换检测缓冲区（Translation Lookaside Buffer, TLB），有时又称为相联存储器（associate memory）或快表。它通常在内存管理单元MMU中，包含少量的表项，在此例中为8个，在实际中很少会超过256个。每个表项记录了一个页面的相关信息，除了虚拟页号，这些域与页表中的域是一一对应的。另外还有一位用来记录这个表项是否有效。

![](/images/现代操作系统/TLB加速分页.png)

这种解决方案是基于一个这样的观察：大多数程序总是对少量的页表进行多次的访问，而不是相反。因此使用硬件存储少量表项可以带来速度的提升。

当程序请求的虚拟页面在转换检测缓冲区中，那么直接使用即可，如果不在，那么从转换检测缓冲区中淘汰一个表项，然后用新找到的页表项替换它。

**软件TLB管理**
许多现代的RISC机器（原文中没有提到x86机器，不知道实际情况是怎样的），几乎所有的页面管理都是在软件中实现的。在这些机器上，TLB表项被操作系统显式地装载，当发生TLB访问失效时，不再是由MMU负责查找，而是由操作系统解决。令人惊奇的是，如果TLB大到（如64个表项）可以减少失效率时，TLB的软件管理就会变得足够有效。当然，所有处理访问失效的操作都必须在有限的几条指令中完成。

### 针对大内存的页表
在原有的内存页表方案之上，引入TLB可以加快虚拟地址到物理地址的转换。另一个问题是怎样处理巨大的虚拟地址空间。

**多级页表**
![](/images/现代操作系统/多级页表.png)

如图所示，32位的虚拟地址被划分为10位的PT1域，10位的PT2域和12位的Offset域。当一个虚拟地址被送到MMU时，MMU首先提取PT1域并把该值作为访问顶级页表的索引，顶级页表每一个表项都表示4M的块地址范围。由索引顶级页表得到的表项中含有二级页表的地址或页框号。在图中例子中，顶级页表的表项0指向程序正文的页表，表项1指向数据的页表，表项1023指向堆栈的页表，其他的页表（用阴影表示的）未使用。通过多级的引用和指向，最终可以将虚拟地址转化为物理地址。
更大的地址空间可以用更多的级数来解决，目前芯片制造者通常使用4级页表来表示256TB的内存空间，这足够使用相当长一段时间，因此芯片制造者没有再多加一层。

**倒排页表**
所谓倒排页表，就是将实际内存中的每一个页框对应一个表项，而不是每个虚拟页面对应一个表项。这样做的好处是节省空间，因为虚拟地址空间比实际内存空间大得多，然而坏处也显而易见，就是从虚拟地址到物理地址的转换会变得困难，因为实现这一过程需要遍历整个倒排页表，这非常低效。
使用TLB可以缓解这一问题。如果TLB能够记录所有频繁使用的页面，那么地址的转换就可能变得像通常的页表一样快。但是当发生TLB失效时，则又需要遍历了。加速遍历的一个可行的方法是建立一张散列表，用虚拟地址来散列（其实就是哈希表）。

## 页面置换算法
当发生缺页中断的时候，操作系统必须在内存中选择一个页面将其换出内存，以便为即将调入的页面腾出空间。选择合适的页面淘汰可以提升系统的性能。本节将会介绍几种这方面的算法。需要指出的是，“页面置换”问题在计算机设计的其他领域中页同样会发生。如高速缓存中内容的替换，web服务器缓存的常用web页面。
另外一个需要考虑的问题是，当需要置换时，它是否只能是缺页进程自己的页面，能不能换其他进程的页面？前者情况中，我们可以有效地将每一个进程限定在固定的页面数目内，而后者不行。

### 最优页面置换算法
在缺页中断发生时，内存中有些页面会在未来10个指令后被访问，有些是100个，1000个。最优页面置换算法将每个页面用这些指令数来标记，当中断发生，置换标记最大的页面。这么做可以尽可能地减少置换的次数。
它唯一的缺点是没法实现。因为操作系统无法预测各个页面下一次将在什么时候被访问。因此该算法只能作为一个标杆去评价其他可以实现的算法。

### 最近未使用页面置换算法
可以使用R(ead)位和M(odification)位来构造一个简单的页面置换算法：当启动一个进程时，它的所有页面的两个位都由操作系统设置为0，R位被定期地清零，当页面被修改过后，M位置1。这样，所有页面被分为4类：
1. 没有被访问，没有被修改
2. 没有被访问，已经被修改
3. 已经被访问，没有被修改
4. 已经被访问，已经被修改
NRU(Not Recently Used, 最近未使用)算法随机地从编号最小的非空类中挑选一个页面淘汰。该算法的优点是易于理解且易于实现。虽然它的性能不是最好的，但已经够用了。

### 先进先出页面置换算法
由操作系统维护一个所有当前在内存中的页面的链表，最新进入的放在表尾，最早进入的放在表头。当缺页中断发生的时候，淘汰表头，并把新的加到表尾。显然，这种算法容易淘汰那些常用的页面导致效率受损，因此很少使用纯粹的FIFO(First-In First-Out)算法。

### 第二次机会页面置换算法
为了避免FIFO算法带来的将经常使用的页面置换出去的问题，第二次机会页面置换算法做了一个简单的修改：检查表头页面的R位，如果是0，就淘汰，如果是1，就将R位清零，放到表尾，然后再检查新的表头。

### 时钟页面置换算法
该算法和第二次机会算法没有啥本质区别，只是将链表首尾相联。

![](/images/现代操作系统/时钟页面置换算法.png)

### 最近最少使用页面置换算法
对最优算法的一个很好的近似是基于这样的观察：在前面几条指令中频繁使用的页面很可能在后面的几条指令中被使用，反过来说，很久没有使用的额页面在未来很长一段时间内可能仍然不会被使用。因此，我们可以在缺页中断发生时，置换未使用时间最长的页面。这个策略称为LRU(Least Recently Used, 最近最少使用)页面置换算法。
虽然LRU在理论上是可以实现的，但代价很高，我们必须维护一个所有页面的数据结构，其中存放页面的最近一次被使用的时间，并且在每次访问内存时更新这个数据结构。

### 用软件模拟LRU
虽然LRU理论上可以实现，但只有非常少的计算机拥有实现它的专门的硬件。一种软件实现方法是，每次时钟中断时，操作系统扫描内存中所有的页面，将每个页面的R位加到它的计数器上，当缺页中断发生时，置换计数器值最小的页面。
这种算法的主要问题是，它会保留历史上最多被访问的页面，而该页面也许在当前和未来不会再被使用了，新被置换进来的页面虽然被使用，但总也比不上老页面，这就导致老页面虽然无用，但还是留在内存中，新页面虽然被使用，但总是被换出去，这不是我们想要的。
因此引入老化(aging)算法。计数器加1时总是加在最高位，并且在时钟中断时先右移一位。

![](/images/现代操作系统/老化.png)

如此一来，旧页面的计数会随着时间的推移减少，不必担心我们之前提到的问题。

### 工作集页面置换算法
这个算法和之前的算法相比，解决的不是单个进程出现缺页中断的问题，而是进程切换时，页面置换的问题。
当进程切换时，如果我们按照之前的页面置换逻辑什么都不做，其实也能运行，但会触发大量的缺页中断，操作系统必须将运行所需的页面一个一个地从磁盘读到内存，导致运行很缓慢，这种现象称为颠簸。
为了解决这个问题，有人提出了工作集模型，就是统计进程运行所需的页面，即工作集，在进程被调回内存时，一次性全部加载，这种方法也称为预先调页(prepaging)。
那么如何确定工作集呢？一种容易实现的方法是，将过去10ms中的进程访问的所有页面作为工作集。为了效率，我们不会每10ms就开辟一块空间将工作集记录下来，而是在固定的空间里，不断淘汰不属于工作集的页面。

![](/images/现代操作系统/工作集算法.png)

定期的时钟中断会清除页面的R位，在缺页中断发生时，扫描页表，R位为1的更新其上次使用时间，R位位0的页面如果上次使用时间到当前时间的差大于我们定义的10ms，则该页面就不属于工作集，可以置换。如果都小于10ms，淘汰生存时间最长的，如果R位都为1，随机淘汰。

### 工作集时钟页面置换算法
原理其实和上一种一样，但是改进了工作集的数据结构，根据页面置换进工作集空间的时间将页框组成一个环状链表，如图。

![](/images/现代操作系统/工作集时钟页面置换算法.png)

因为已经是按时间排序了，所以不必再遍历，提高了速度。此算法在实际工作中得到了广泛应用。

## 分页系统中的设计问题
前几节我们讨论在发生缺页中断时用来选择页面淘汰的几个算法，向上一层看，或者说更宏观一点来看，会遇到一个问题：怎样在相互竞争的可运行进程之间分配内存。
### 局部分配策略与全局分配策略
假设有A、B、C三个可运行的进程，当A发生缺页中断时，页面置换算法在选择淘汰页面时是只考虑分配给A的6个页面还是考虑所有在内存中的页面？前者称为局部页面置换，后者称为全局页面置换。

![](/images/现代操作系统/局部页面置换和全局页面置换.png)

通常情况下，全局算法工作得比局部算法好。

### 负载控制
一旦所有进程的组合工作集超出了内存容量，即使是使用最优页面置换算法，系统也可能发生颠簸。唯一现实的解决方法是暂时从内存中去掉一些进程，这些进程可以被交换到磁盘，并释放他们所占有的页面。因此，即使是使用分页，交换也是需要的，只是现在交换是用来减少对内存潜在的需求，而不是收回它的页面。
另一个需要考虑的因素是cpu的使用率，算法需要根据进程的特性（cpu密集型还是i/o密集型）来选择交换出去的进程。

### 页面大小
要确定最佳的页面大小需要在几个互相矛盾的因素之间进行权衡，从结果看，不存在全局最优。
小页面可以减少浪费的内存，但相对的，它需要更大的页表，因为它数量更多。在内存和磁盘之间交换页面大部分的时间都花在了寻道和旋转延迟上，所以传输一个小页面和传输一个大页面基本上耗时是相同的，因此大页面在这方面更优。

### 共享页面
当多个用户同时运行一个程序时，使用共享页面效率更高。那些只读的页面（如程序文本）可以共享，但是数据页面则不能共享。需要注意的是，共享页面需要特殊的页表来避免当A、B进程共享某些页面时，调度程序决定调走A并交换其在内存中占有的页面导致进程B产生大量缺页中断的问题。
在UNIX中，执行fork调用通常让父子进程分别拥有自己的页表，但都指向同一个页面集合，当父子页面仅仅是读数据时，可以避免复制。一旦有进程更新了数据，就会触发只读保护，操作系统会将被修改的页面进行复制。这种方法称为写时复制，它通过减少复制而提高了性能。

### 共享库
在Windows上被称为DLL，当多个程序使用了相同的DLL，实际上只有一份副本在内存中，且DLL的加载并不是被一次性地读入，而是根据需要，以页面为单位装载的。

### 内存映射文件
进程可以通过发起一个系统调用，将一个文件映射到其虚拟地址空间的一部分，当有进程访问某个页面时，该页面才会被加载到内存中。当进程退出或显式地接触文件映射时，被改动的页面才会被写回到磁盘文件中。程序员利用这种机制，在进程间实现通信。共享库是内存映射文件的一种特例。

### 清除策略
为保证有足够的空闲页框，很多分页系统有一个称为分页守护进程(paging daemon)的后台进程，大多数时候睡眠，但定期被唤醒以检查内存的状态，如果空闲页框过少，则会淘汰一些页面。

### 虚拟内存接口
虚拟内存接口允许程序员指定一块内存区域被不同的进程知道，从而实现共享内存，从而实现进程间通信。
另一种高级存储管理技术是分布式共享内存，缺页中断发生时，可以通过网络去寻找其他物理内存的页框。

## 分段
段(segment)是一个逻辑实体，由若干个页面组成。每个段都构成了一个独立的地址空间，他们可以独立地增长或减小而不会影响到其他的段，这就解决了在内存上分配会变化的空间的问题。

![](/images/现代操作系统/分段存储管理.png)

这么做的好处还有可以简化链接，因为每个过程都位于一个独立的段中并且起始地址都是0，并且在程序修改并重新编译后，未更改的段的地址不会因为其他地方的修改而改变，因此不用重新链接。
分段也有助于进程间共享过程和数据。共享库可以放到单独的段中由各个进程共享而不必在每个进程的地址空间中都保存一份。

### 分段和分页的结合：MULTICS
MULTICS是有史以来最具影响力的操作系统之一，最后一个MULTICS系统在运行了31年后于2000年关闭。几乎没有其他操作系统能像MULTICS一样几乎没有修改地持续运行这么长时间。
MULTICS的设计者把每个段都看作一个虚拟内存并对它进行分页。因此MULTICS中一个地址由2个部分构成：段号和段内地址，段内地址又分为页号和页内偏移量。

![](/images/现代操作系统/MULTICS虚拟地址.png)

MULTICS硬件包含了16个字的高速TLB，以此提高寻址效率。

### 分段和分页的结合：Intel x86
很长很复杂，但已知的是，从x86_64起，除了在传统模式下，分段机制已被认为是过时的且不再被支持。


## 后记
这一章写的时间格外的长，写到我本人都辞职回家了，但内容还是十分重要的，内存的管理实际上体现了很多计算机的设计原则，它们是通用的，即使对于纯软件的编程来说也是很有参考意义的。

2020/10/24
